{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Hand Crafted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necesssary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images from the dataset are read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to dataset\n",
    "dataset_path = r\"dataset\"  # Using raw strings bacause the file names contain backslashes and escape characters\n",
    "mask_path = os.path.join(dataset_path, \"with_mask\")\n",
    "no_mask_path = os.path.join(dataset_path, \"without_mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image not loaded correctly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_26472\\2602037394.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  sample_image = cv2.imread('classification_dataset\\with_mask\\0_0_‚âàÀô‚óä¬¢ 2020-02-23 132115.png')\n"
     ]
    }
   ],
   "source": [
    "#loading a sample image to show that images with 0_0_‚âàÀô‚óä¬¢ are not loading correctly\n",
    "sample_image = cv2.imread('classification_dataset\\with_mask\\0_0_‚âàÀô‚óä¬¢ 2020-02-23 132115.png')\n",
    "if sample_image is None:\n",
    "    print('Image not loaded correctly')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error shows that the images with this kind of a name are not being loaded. Manual inspection shows that these kind of images only exist in the with_mask category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a python script to rename all the wrongly names images\n",
    "import os\n",
    "\n",
    "# Define the folder path where images are stored\n",
    "folder_path = mask_path\n",
    "\n",
    "# Define the characters to be replaced\n",
    "special_chars = \"‚âàÀô‚óä¬¢\"\n",
    "\n",
    "# List all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    old_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Replace specific special characters with '_'\n",
    "    new_filename = filename\n",
    "    for char in special_chars:\n",
    "        new_filename = new_filename.replace(char, \"_\")\n",
    "\n",
    "    new_path = os.path.join(folder_path, new_filename)\n",
    "\n",
    "    # Rename the file if necessary\n",
    "    if old_path != new_path:\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"Renamed: {filename} ‚Üí {new_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the images can be loaded without issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction functions\n",
    "def ExtractHogFeatures(img):\n",
    "    grayImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    hogFeatures, _ = hog(grayImg, pixels_per_cell=(8, 8), cells_per_block=(2, 2), \n",
    "                         block_norm='L2-Hys', visualize=True)\n",
    "    return hogFeatures\n",
    "\n",
    "def ExtractLbpFeatures(img):\n",
    "    grayImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    lbpImage = local_binary_pattern(grayImg, P=8, R=1, method=\"uniform\")\n",
    "    histValues, _ = np.histogram(lbpImage.ravel(), bins=np.arange(0, 10), range=(0, 10))\n",
    "    histValues = histValues.astype(\"float\")\n",
    "    histValues /= histValues.sum()\n",
    "    return histValues\n",
    "\n",
    "def ExtractColorHistogram(img, bins=(8, 8, 8)):\n",
    "    hsvImg = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    colorHist = cv2.calcHist([hsvImg], [0, 1, 2], None, bins, [0, 180, 0, 256, 0, 256])\n",
    "    colorHist = cv2.normalize(colorHist, colorHist).flatten()\n",
    "    return colorHist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features and storing them in numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image: dataset\\with_mask\\0_0_≈ì¬¨‚Äò√ø.png\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and extract features\n",
    "FeatureList, LabelList = [], []\n",
    "\n",
    "for ClassLabel, ClassPath in enumerate([mask_path, no_mask_path]):  # 0: with_mask, 1: without_mask\n",
    "    for FileName in os.listdir(ClassPath):\n",
    "        ImgPath = r\"{}\".format(os.path.join(ClassPath, FileName))  # Use raw string path\n",
    "        InputImage = cv2.imread(ImgPath)\n",
    "        if InputImage is not None:\n",
    "            InputImage = cv2.resize(InputImage, (128, 128))\n",
    "            # Extract features\n",
    "            HogFeat = ExtractHogFeatures(InputImage)\n",
    "            LbpFeat = ExtractLbpFeatures(InputImage)\n",
    "            ColorFeat = ExtractColorHistogram(InputImage)\n",
    "            # Combine features\n",
    "            CombinedFeatures = np.hstack([HogFeat, LbpFeat, ColorFeat])\n",
    "            FeatureList.append(CombinedFeatures)\n",
    "            LabelList.append(ClassLabel)\n",
    "        else:\n",
    "            print(f\"Error loading image: {ImgPath}\")\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(FeatureList)\n",
    "y = np.array(LabelList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data is split into 3 parts - Train (70%), Validation (15%), Test (15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Simplified Train-Validation-Test Split (70%-15%-15%)**\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, train_size=0.7, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and training the three ML classifiers (Using hand crafted features)\n",
    "- Random Forest\n",
    "- SVM\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest F1 Score: 0.9228\n",
      "Classification Report (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94       334\n",
      "           1       0.95      0.90      0.92       280\n",
      "\n",
      "    accuracy                           0.93       614\n",
      "   macro avg       0.93      0.93      0.93       614\n",
      "weighted avg       0.93      0.93      0.93       614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest classifier\n",
    "RandomForest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "RandomForest.fit(X_train, y_train)\n",
    "RfValPreds = RandomForest.predict(X_val)\n",
    "RfF1Score = f1_score(y_val, RfValPreds)\n",
    "print(f\"Random Forest F1 Score: {RfF1Score:.4f}\")\n",
    "print(\"Classification Report (Random Forest):\")\n",
    "print(classification_report(y_val, RfValPreds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM F1 Score: 0.9250\n",
      "Classification Report (SVM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94       334\n",
      "           1       0.93      0.93      0.93       280\n",
      "\n",
      "    accuracy                           0.93       614\n",
      "   macro avg       0.93      0.93      0.93       614\n",
      "weighted avg       0.93      0.93      0.93       614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train SVM classifier\n",
    "SvmClassifier = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "SvmClassifier.fit(X_train, y_train)\n",
    "SvmValPreds = SvmClassifier.predict(X_val)\n",
    "SvmF1Score = f1_score(y_val, SvmValPreds)\n",
    "print(f\"SVM F1 Score: {SvmF1Score:.4f}\")\n",
    "print(\"Classification Report (SVM):\")\n",
    "print(classification_report(y_val, SvmValPreds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost F1 Score: 0.9236\n",
      "Classification Report (XGBoost):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94       334\n",
      "           1       0.94      0.91      0.92       280\n",
      "\n",
      "    accuracy                           0.93       614\n",
      "   macro avg       0.93      0.93      0.93       614\n",
      "weighted avg       0.93      0.93      0.93       614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost Classifier\n",
    "XgbClassifier = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "XgbClassifier.fit(X_train, y_train)\n",
    "XgbValPreds = XgbClassifier.predict(X_val)\n",
    "XgbF1Score = f1_score(y_val, XgbValPreds)\n",
    "print(f\"XGBoost F1 Score: {XgbF1Score:.4f}\")\n",
    "print(\"Classification Report (XGBoost):\")\n",
    "print(classification_report(y_val, XgbValPreds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores are calculated and the best model is written as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1-Scores:\n",
      "  - SVM: 0.9250\n",
      "  - Random Forest: 0.9228\n",
      "  - XGBoost: 0.9236\n",
      "\n",
      "Best Model: SVM (Validation F1-Score = 0.9250)\n",
      "Test F1-Score for Best Model (SVM): 0.8917\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       336\n",
      "           1       0.90      0.89      0.89       279\n",
      "\n",
      "    accuracy                           0.90       615\n",
      "   macro avg       0.90      0.90      0.90       615\n",
      "weighted avg       0.90      0.90      0.90       615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare validation F1-scores of all models\n",
    "ModelScores = {\n",
    "    \"SVM\": (SvmClassifier, SvmF1Score),\n",
    "    \"Random Forest\": (RandomForest, RfF1Score),\n",
    "    \"XGBoost\": (XgbClassifier, XgbF1Score)\n",
    "}\n",
    "\n",
    "# Print individual validation F1-scores\n",
    "print(\"Validation F1-Scores:\")\n",
    "for modelName, (_, f1ScoreVal) in ModelScores.items():\n",
    "    print(f\"  - {modelName}: {f1ScoreVal:.4f}\")\n",
    "\n",
    "# Choose best model based on validation F1-score\n",
    "BestModelName, (BestModel, BestF1Score) = max(ModelScores.items(), key=lambda x: x[1][1])\n",
    "\n",
    "# Final testing on the best model\n",
    "FinalTestPreds = BestModel.predict(X_test)\n",
    "FinalTestF1 = f1_score(y_test, FinalTestPreds)\n",
    "FinalTestReport = classification_report(y_test, FinalTestPreds)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nBest Model: {BestModelName} (Validation F1-Score = {BestF1Score:.4f})\")\n",
    "print(f\"Test F1-Score for Best Model ({BestModelName}): {FinalTestF1:.4f}\")\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(FinalTestReport)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, out of the 3 chosen ML classifiers, SVM performs the best at classifying the image dataset using hand crafted features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using CNN(Automatic Feature Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing involves renaming file names uniformly accross both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(dataset_path,string):\n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "    \n",
    "    # Rename files sequentially\n",
    "    for index, filename in enumerate(image_files, start=1):\n",
    "        old_path = os.path.join(dataset_path, filename)\n",
    "\n",
    "        # Extract the file extension (e.g., .jpg, .png)\n",
    "        extension = os.path.splitext(filename)[1]  # Includes the dot\n",
    "\n",
    "        # Generate new filename\n",
    "        new_filename = f\"image_{string}_{index}{extension}\"\n",
    "        new_path = os.path.join(dataset_path, new_filename)\n",
    "\n",
    "        # Rename the file\n",
    "        os.rename(old_path, new_path)\n",
    "        # print(f\"Renamed: {filename} ‚Üí {new_filename}\")\n",
    "\n",
    "\n",
    "rename(r\"dataset/with_mask\", 'with_mask')\n",
    "rename(r\"dataset/without_mask\", 'without_mask')\n",
    "\n",
    "print(\" All images renamed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "from torch.optim import Adam, RMSprop\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setting up GPU (this was run on Kaggle for convenience and faster results)\n",
    "- Setting up the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}, GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "dataset_path = \"/kaggle/input/dataset\"\n",
    "img_size = (128, 128)\n",
    "batch_sizes = [16, 32]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "optimizers = [Adam, RMSprop]\n",
    "activations = ['relu', 'tanh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created 2 CNN classes - one for ReLU and one for Tanh activation functions, we were facing some issue in passing it as a parameter to the constructor of this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNReLU, self).__init__()\n",
    "        act = nn.ReLU\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), act(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), act(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), act(), nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 128), act(),\n",
    "            nn.Linear(128, 1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "\n",
    "class CNNTanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNTanh, self).__init__()\n",
    "        act = nn.Tanh\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), act(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), act(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), act(), nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 128), act(),\n",
    "            nn.Linear(128, 1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data is split into 3 parts - Train (70%), Validation (15%), Test (15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(img_size), transforms.ToTensor()])\n",
    "full_dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "\n",
    "train_len = int(0.7 * len(full_dataset))\n",
    "val_len = int(0.15 * len(full_dataset))\n",
    "test_len = len(full_dataset) - train_len - val_len\n",
    "train_set, val_set, test_set = random_split(full_dataset, [train_len, val_len, test_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created function to create model, run and calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, optimizer, epochs=15):\n",
    "    criterion = nn.BCELoss()\n",
    "    model = model.to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False):\n",
    "            x, y = x.to(device), y.float().unsqueeze(1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            outputs = model(x)\n",
    "            preds = (outputs.cpu().numpy() > 0.5).astype(int)\n",
    "            y_pred.extend(preds.flatten())\n",
    "            y_true.extend(y.numpy())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final output cell, calls function in a loop and trains model for each parameter combination. It stores result in a list which is later used to compare and find the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for opt_class in optimizers:\n",
    "            for act in activations:\n",
    "                print(f\"\\nüîÅ Training with LR={lr}, Batch={batch_size}, Optimizer={opt_class.__name__}, Activation={act}\")\n",
    "\n",
    "                if act == 'relu':\n",
    "                    model = CNNReLU()\n",
    "                elif act == 'tanh':\n",
    "                    model = CNNTanh()\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "                val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "                test_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "                optimizer = opt_class(model.parameters(), lr=lr)\n",
    "\n",
    "                acc, f1 = train_and_evaluate(model, train_loader, val_loader, test_loader, optimizer)\n",
    "\n",
    "                all_results.append({\n",
    "                    \"Learning Rate\": lr,\n",
    "                    \"Batch Size\": batch_size,\n",
    "                    \"Optimizer\": opt_class.__name__,\n",
    "                    \"Activation\": act,\n",
    "                    \"Test Accuracy\": acc,\n",
    "                    \"Test F1-Score\": f1\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of result and comparative overview of the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.sort_values(by=\"Test F1-Score\", ascending=False, inplace=True)\n",
    "print(results_df.head())\n",
    "\n",
    "best_model = results_df.iloc[0]\n",
    "print(\"\\nBest CNN Configuration:\")\n",
    "for col in best_model.index:\n",
    "    print(f\"{col}: {best_model[col]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the CNN based classifier outperforms the ML based classifier by a significant margin, as shown by the accuracy scores calculated by testing on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
